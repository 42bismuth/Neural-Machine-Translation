{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb49525",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "This project focuses on building an English-to-Portuguese Neural Machine Translation (NMT) model using Long Short-Term Memory (LSTM) networks with attention. Machine translation is a significant task in natural language processing, with applications not only in translating languages but also in word sense disambiguation (e.g., determining whether \"bank\" refers to a financial institution or the side of a river). While Recurrent Neural Networks (RNNs) with LSTMs can handle short to medium-length sentences, they often struggle with very long sequences due to vanishing gradients. To mitigate this, an attention mechanism is introduced, allowing the decoder to access all relevant parts of the input sentence, regardless of its length.\n",
    "\n",
    "The project involves:\n",
    "\n",
    "1.Implementing an encoder-decoder system with attention.\n",
    "2.Building the NMT model from scratch using TensorFlow.\n",
    "3.Generating translations using greedy and Minimum Bayes Risk (MBR) decoding.\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Data Preparation](#1)\n",
    "- [2 - NMT model with attention](#2)\n",
    "- [3 - Training](#3)\n",
    "- [4 - Using the model for inference ](#4)\n",
    "- [5 - Minimum Bayes-Risk Decoding](#5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ef370d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Setting this env variable prevents TF warnings from showing up\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from utils import (sentences, train_data, val_data, english_vectorizer, portuguese_vectorizer, \n",
    "                   masked_loss, masked_acc, tokens_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adb8fd6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import w1_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76be1dc",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1. Data Preparation\n",
    "\n",
    "The text pre-processing bits have already been taken care of (if you are interested in this be sure to check the `utils.py` file). The steps performed can be summarized as:\n",
    "\n",
    "- Reading the raw data from the text files\n",
    "- Cleaning the data (using lowercase, adding space around punctuation, trimming whitespaces, etc)\n",
    "- Splitting it into training and validation sets\n",
    "- Adding the start-of-sentence and end-of-sentence tokens to every sentence\n",
    "- Tokenizing the sentences\n",
    "- Creating a Tensorflow dataset out of the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226033a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (to translate) sentence:\n",
      "\n",
      "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n",
      "\n",
      "Portuguese (translation) sentence:\n",
      "\n",
      "Não importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n"
     ]
    }
   ],
   "source": [
    "portuguese_sentences, english_sentences = sentences\n",
    "\n",
    "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
    "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba90eb9",
   "metadata": {},
   "source": [
    "I don't have much use for the raw sentences so delete them to save memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f081b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "del portuguese_sentences\n",
    "del english_sentences\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff83d2",
   "metadata": {},
   "source": [
    "Notice that an `english_vectorizer` and a `portuguese_vectorizer` is imported from `utils.py`. These are created using [tf.keras.layers.TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) and they provide interesting features such as ways to visualize the vocabulary and convert text into tokenized ids and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1cfc17",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words of the english vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'i', 'to', 'you', 'the']\n",
      "\n",
      "First 10 words of the portuguese vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'que', 'o', 'nao', 'eu']\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 10 words of the english vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
    "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{portuguese_vectorizer.get_vocabulary()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152b075",
   "metadata": {},
   "source": [
    "Notice that the first 4 words are reserved for special words. In order, these are:\n",
    "\n",
    "- the empty string\n",
    "- a special token to represent an unknown word\n",
    "- a special token to represent the start of a sentence\n",
    "- a special token to represent the end of a sentence\n",
    "\n",
    "You can see how many words are in a vocabulary by using the `vocabulary_size` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5facaa0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese vocabulary is made up of 12000 words\n",
      "English vocabulary is made up of 12000 words\n"
     ]
    }
   ],
   "source": [
    "# Size of the vocabulary\n",
    "vocab_size_por = portuguese_vectorizer.vocabulary_size()\n",
    "vocab_size_eng = english_vectorizer.vocabulary_size()\n",
    "\n",
    "print(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\n",
    "print(f\"English vocabulary is made up of {vocab_size_eng} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4b615",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "I have defined [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) objects that will help you map from words to ids and vice versa. Same for the portuguese vocabulary since this will be useful later on when I decode the predictions from your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218f7a36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This helps you convert from words to ids\n",
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(), \n",
    "    mask_token=\"\", \n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "# This helps you convert from ids to words\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\",\n",
    "    invert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20076b9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id for the [UNK] token is 1\n",
      "The id for the [SOS] token is 2\n",
      "The id for the [EOS] token is 3\n",
      "The id for baunilha (vanilla) is 7079\n"
     ]
    }
   ],
   "source": [
    "unk_id = word_to_id(\"[UNK]\")\n",
    "sos_id = word_to_id(\"[SOS]\")\n",
    "eos_id = word_to_id(\"[EOS]\")\n",
    "baunilha_id = word_to_id(\"baunilha\")\n",
    "\n",
    "print(f\"The id for the [UNK] token is {unk_id}\")\n",
    "print(f\"The id for the [SOS] token is {sos_id}\")\n",
    "print(f\"The id for the [EOS] token is {eos_id}\")\n",
    "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d744c",
   "metadata": {},
   "source": [
    "Finally, the data that is going to be fed to the neural network. Both `train_data` and `val_data` are of type `tf.data.Dataset` and are already arranged in batches of 64 examples. To get the first batch out of a tf dataset you can use the `take` method. To get the first example out of the batch, slice the tensor and use the `numpy` method for nicer printing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739777eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized english sentence:\n",
      "[   2  210    9  146  123   38    9 1672    4    3    0    0    0    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence (shifted to the right):\n",
      "[   2 1085    7  128   11  389   37 2038    4    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence:\n",
      "[1085    7  128   11  389   37 2038    4    3    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (to_translate, sr_translation), translation in train_data.take(1):\n",
    "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9ee3c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Padding has already been applied to the tensors and the value used for this is 0\n",
    "- Each example consists of 3 different tensors:\n",
    "    - The sentence to translate\n",
    "    - The shifted-to-the-right translation\n",
    "    - The translation\n",
    "    \n",
    "The first two can be considered as the features, while the third one as the target. By doing this your model can perform Teacher Forcing as you saw in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41cb52",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here’s the rewritten version in the third person:\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"2\"></a>\n",
    "## 2. NMT Model with Attention\n",
    "\n",
    "This project involves building an encoder-decoder architecture for a Neural Machine Translation (NMT) model. The model uses a Recurrent Neural Network (RNN) that processes a tokenized sentence in the encoder, which is then passed to the decoder for translation. As discussed in the background, a regular sequence-to-sequence model with LSTMs is effective for short to medium sentences but tends to degrade in performance with longer sentences. This degradation occurs because all the context of the input sentence is compressed into a single vector passed to the decoder block, as illustrated below. For very long sentences (e.g., 100 tokens or more), the early parts of the input have minimal impact on the final vector passed to the decoder, leading to translation issues.\n",
    "\n",
    "<img src='images/plain_rnn.png'>\n",
    "\n",
    "To address this issue, an attention layer is added to the model, allowing the decoder to access all parts of the input sentence. Consider a 4-word input sentence as shown below. A hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These hidden states are passed to the attention layer, where each is scored based on the current activation (i.e., hidden state) of the decoder. For instance, after the first prediction, \"como,\" is made, the attention layer receives all the encoder hidden states (orange rectangles) and the decoder hidden state corresponding to \"como\" (first green rectangle). Based on this, it scores the encoder hidden states to determine which should be emphasized for the next word prediction. Through training, the model may learn to align with the second encoder hidden state, assigning a high probability to the word \"você.\" In greedy decoding, this word is output as the next symbol, and the process repeats until an end-of-sentence prediction is reached.\n",
    "\n",
    "<img src='images/attention_overview.png'>\n",
    "\n",
    "This project uses Scaled Dot Product Attention, defined as:\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "This equation will be explored in more detail later, but for now, it can be understood as computing scores using queries (Q) and keys (K), followed by multiplying the values (V) to obtain a context vector at each timestep of the decoder. This context vector is fed into the decoder RNN to generate a set of probabilities for the next predicted word. The division by the square root of the keys' dimensionality ($\\sqrt{d_k}$) enhances model performance, which will be further discussed later. In this machine translation task, the encoder activations (encoder hidden states) serve as the keys and values, while the decoder activations (decoder hidden states) serve as the queries.\n",
    "\n",
    "Despite the complexity of this architecture, the implementation can be achieved with just a few lines of code.\n",
    "\n",
    "Two important global variables are defined:\n",
    "\n",
    "- The size of the vocabulary.\n",
    "- The number of units in the LSTM layers (consistent across all LSTM layers).\n",
    "\n",
    "For this project, the vocabulary sizes for English and Portuguese are the same, so a single constant, `VOCAB_SIZE`, is used throughout. In other settings, vocabulary sizes might differ, but that is not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e484abf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc251965",
   "metadata": {},
   "source": [
    "Here’s the rewritten version in the third person:\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"ex1\"></a>\n",
    "## Encoder\n",
    "\n",
    "The first part of the project involves implementing the encoder component of the neural network. The `Encoder` class will be completed by defining the necessary sublayers in the constructor (`__init__` method) and then utilizing these sublayers in the forward pass (`call` method).\n",
    "\n",
    "The encoder consists of the following layers:\n",
    "\n",
    "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): This layer requires the definition of appropriate `input_dim` and `output_dim` values. Additionally, it needs to be informed that '0' is used for padding, which can be achieved by setting the `mask_zero` parameter accordingly.\n",
    "\n",
    "- [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): TensorFlow provides functionality for implementing bidirectional behavior in RNN-like layers. While the bidirectional part is pre-configured, specifying the correct layer type and parameters is necessary. The number of units must be set appropriately, and the LSTM should return the full sequence rather than just the last output, which can be controlled using the `return_sequences` parameter.\n",
    "\n",
    "The forward pass will be defined using TensorFlow's [functional API](https://www.tensorflow.org/guide/keras/functional_api), which allows for chaining function calls to construct the network, as shown in the following example:\n",
    "\n",
    "```python\n",
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1db0a1d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(  \n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        self.rnn = tf.keras.layers.Bidirectional(  \n",
    "            merge_mode=\"sum\",  \n",
    "            layer=tf.keras.layers.LSTM(\n",
    "                units=units,\n",
    "                return_sequences= True\n",
    "            ),  \n",
    "        )  \n",
    "\n",
    "    def call(self, context):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): The sentence to translate\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Encoded sentence to translate\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.embedding(context)\n",
    "\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65034ffd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences in english has shape: (64, 14)\n",
      "\n",
      "Encoder output has shape: (64, 14, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "encoder_output = encoder(to_translate)\n",
    "\n",
    "print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
    "print(f'Encoder output has shape: {encoder_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe83f4",
   "metadata": {},
   "source": [
    "Here’s the rewritten version in the third person while keeping the content and word count similar:\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"ex2\"></a>\n",
    "## CrossAttention\n",
    "\n",
    "The next part of the project involves implementing the layer that performs cross-attention between the original sentences and their translations. The `CrossAttention` class will be completed by defining the necessary sublayers in the constructor (`__init__` method) and then utilizing these sublayers in the forward pass (`call` method). Some elements of this process are already provided.\n",
    "\n",
    "The cross-attention layer includes the following components:\n",
    "\n",
    "- [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention): This layer requires defining the appropriate `key_dim`, which represents the size of the key and query tensors. Additionally, the number of heads should be set to 1, as the attention is between two tensors rather than multi-head attention. This layer is preferred over [Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) due to its simpler implementation during the forward pass.\n",
    "\n",
    "Key details to note:\n",
    "- The output of the attention mechanism needs to be combined with the shifted-to-the-right translation (since this cross-attention occurs on the decoder side). This is done using an [Add](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add) layer to maintain the original dimensionality, which wouldn't be preserved with a [Concatenate](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate) layer.\n",
    "\n",
    "- For enhanced network stability, [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization) is also applied.\n",
    "\n",
    "There is no need to address these final steps, as they have already been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e71f3d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.mha = ( \n",
    "            tf.keras.layers.MultiHeadAttention(\n",
    "                key_dim=units,\n",
    "                num_heads=1\n",
    "            ) \n",
    "        )  \n",
    "\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, context, target):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The embedded shifted-to-the-right translation\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Cross attention between context and target\n",
    "        \"\"\"\n",
    "    \n",
    "        # Call the MH attention by passing in the query and value\n",
    "        # For this case the query should be the translation and the value the encoded sentence to translate\n",
    "        attn_output = self.mha(\n",
    "            query=target,\n",
    "            value=context\n",
    "        )  \n",
    "\n",
    "\n",
    "        x = self.add([target, attn_output])\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c62796f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of contexts has shape: (64, 14, 256)\n",
      "Tensor of translations has shape: (64, 15, 256)\n",
      "Tensor of attention scores has shape: (64, 15, 256)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "sr_translation_embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
    "\n",
    "attention_result = attention_layer(encoder_output, sr_translation_embed)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
    "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4f99a",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Tensor of contexts has shape: (64, 14, 256)\n",
    "Tensor of translations has shape: (64, 15, 256)\n",
    "Tensor of attention scores has shape: (64, 15, 256)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa296ee2",
   "metadata": {},
   "source": [
    "Here’s the rewritten version in the third person:\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"ex3\"></a>\n",
    "## Decoder\n",
    "\n",
    "The next part of the project involves implementing the decoder component of the neural network by completing the `Decoder` class. The constructor (`__init__` method) is used to define all the sublayers of the decoder, and these sublayers are utilized during the forward pass (`call` method).\n",
    "\n",
    "The decoder consists of the following layers:\n",
    "\n",
    "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): The `input_dim` and `output_dim` must be defined for this layer, with padding indicated by using '0'. The `mask_zero` parameter can be set accordingly.\n",
    "\n",
    "- Pre-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): In contrast to the encoder, which employs a Bidirectional LSTM, the decoder uses a standard LSTM. The appropriate number of units should be set, and the LSTM must return the full sequence, not just the last output, by using the `return_sequences` parameter. The layer must also return the state, which is essential for inference, by setting the `return_state` parameter. The LSTM layers return state as a tuple of two tensors traditionally called `memory_state` and `carry_state`, but here, they are referred to as `hidden_state` and `cell_state` to align with lecture terminology.\n",
    "\n",
    "- The attention layer: This layer performs cross-attention between the sentence to be translated and the right-shifted translation. The `CrossAttention` layer from the previous exercise is used here.\n",
    "\n",
    "- Post-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Another LSTM layer is included, though it is not required to return the state.\n",
    "\n",
    "- [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer: This layer should have the same number of units as the vocabulary size since it computes the logits for each possible word in the vocabulary. The `logsoftmax` activation function is applied here, which can be implemented using [tf.nn.log_softmax](https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9639bdb",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )  \n",
    "\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True\n",
    "        )  \n",
    "\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=vocab_size,\n",
    "            activation=tf.nn.log_softmax\n",
    "        )  \n",
    "\n",
    "\n",
    "    def call(self, context, target, state=None, return_state=False):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The shifted-to-the-right translation\n",
    "            state (list[tf.Tensor, tf.Tensor], optional): Hidden state of the pre-attention LSTM. Defaults to None.\n",
    "            return_state (bool, optional): If set to true return the hidden states of the LSTM. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.embedding(target)\n",
    "\n",
    "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
    "\n",
    "        x = self.attention(context, x)\n",
    "\n",
    "        x = self.post_attention_rnn(x)\n",
    "\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "\n",
    "        if return_state:\n",
    "            return logits, [hidden_state, cell_state]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6165cf2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of contexts has shape: (64, 14, 256)\n",
      "Tensor of right-shifted translations has shape: (64, 15)\n",
      "Tensor of logits has shape: (64, 15, 12000)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "logits = decoder(encoder_output, sr_translation)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b5d7d",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Tensor of contexts has shape: (64, 14, 256)\n",
    "Tensor of right-shifted translations has shape: (64, 15)\n",
    "Tensor of logits has shape: (64, 15, 12000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcce3a7",
   "metadata": {},
   "source": [
    "<a name=\"ex4\"></a>\n",
    "## Translator\n",
    "\n",
    "In this section, the layers previously developed are combined into a complete model. This involves completing the `Translator` class. Unlike the `Encoder` and `Decoder` classes, which inherit from `tf.keras.layers.Layer`, the `Translator` class inherits from `tf.keras.Model`.\n",
    "\n",
    "It is important to note that `train_data` yields a tuple consisting of the sentence to be translated and the shifted-to-the-right translation, which serve as the \"features\" of the model. Consequently, the network's inputs will be tuples containing both context and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "205fcf31",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, units)\n",
    "\n",
    "        self.decoder = Decoder(vocab_size, units)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            inputs (tuple(tf.Tensor, tf.Tensor)): Tuple containing the context (sentence to translate) and the target (shifted-to-the-right translation)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "\n",
    "        context, target = inputs\n",
    "\n",
    "        encoded_context = self.encoder(context)\n",
    "\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d4a231c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences to translate has shape: (64, 14)\n",
      "Tensor of right-shifted translations has shape: (64, 15)\n",
      "Tensor of logits has shape: (64, 15, 12000)\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(VOCAB_SIZE, UNITS)\n",
    "\n",
    "# Compute the logits for every word in the vocabulary\n",
    "logits = translator((to_translate, sr_translation))\n",
    "\n",
    "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bc228",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Training\n",
    "\n",
    "Now that there is an untrained instance of the NMT model, it is time to train it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a61ef65",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
    "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=50,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87d9bf9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 45s 61ms/step - loss: 0.8244 - masked_acc: 0.8064 - masked_loss: 0.8251 - val_loss: 1.0437 - val_masked_acc: 0.7786 - val_masked_loss: 1.0446\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 17s 34ms/step - loss: 0.8404 - masked_acc: 0.8043 - masked_loss: 0.8411 - val_loss: 1.0078 - val_masked_acc: 0.7838 - val_masked_loss: 1.0076\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 0.8391 - masked_acc: 0.8050 - masked_loss: 0.8398 - val_loss: 1.0200 - val_masked_acc: 0.7796 - val_masked_loss: 1.0211\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 0.8237 - masked_acc: 0.8068 - masked_loss: 0.8243 - val_loss: 0.9804 - val_masked_acc: 0.7840 - val_masked_loss: 0.9818\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.7970 - masked_acc: 0.8107 - masked_loss: 0.7976 - val_loss: 0.9708 - val_masked_acc: 0.7863 - val_masked_loss: 0.9712\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.6790 - masked_acc: 0.8280 - masked_loss: 0.6796 - val_loss: 0.9605 - val_masked_acc: 0.7906 - val_masked_loss: 0.9639\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.6962 - masked_acc: 0.8240 - masked_loss: 0.6969 - val_loss: 0.9951 - val_masked_acc: 0.7851 - val_masked_loss: 0.9971\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.7120 - masked_acc: 0.8209 - masked_loss: 0.7127 - val_loss: 0.9635 - val_masked_acc: 0.7870 - val_masked_loss: 0.9629\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.7093 - masked_acc: 0.8216 - masked_loss: 0.7100 - val_loss: 0.9476 - val_masked_acc: 0.7931 - val_masked_loss: 0.9479\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 0.6602 - masked_acc: 0.8297 - masked_loss: 0.6608 - val_loss: 0.9362 - val_masked_acc: 0.7998 - val_masked_loss: 0.9376\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.6092 - masked_acc: 0.8395 - masked_loss: 0.6097 - val_loss: 0.9351 - val_masked_acc: 0.7935 - val_masked_loss: 0.9371\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.6223 - masked_acc: 0.8354 - masked_loss: 0.6231 - val_loss: 0.9309 - val_masked_acc: 0.7960 - val_masked_loss: 0.9312\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 0.6345 - masked_acc: 0.8339 - masked_loss: 0.6350 - val_loss: 0.9360 - val_masked_acc: 0.7935 - val_masked_loss: 0.9361\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.6446 - masked_acc: 0.8312 - masked_loss: 0.6451 - val_loss: 0.9359 - val_masked_acc: 0.7974 - val_masked_loss: 0.9350\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.5707 - masked_acc: 0.8459 - masked_loss: 0.5712 - val_loss: 0.9503 - val_masked_acc: 0.7948 - val_masked_loss: 0.9502\n"
     ]
    }
   ],
   "source": [
    "# Train the translator (this takes some minutes so feel free to take a break)\n",
    "\n",
    "trained_translator, history = compile_and_train(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b9301",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4. Using the model for inference \n",
    "\n",
    "\n",
    "Now that your model is trained you can use it for inference. To help you with this the `generate_next_token` function is provided. Notice that this function is meant to be used inside a for-loop, so you feed to it the information of the previous step to generate the information of the next step. In particular you need to keep track of the state of the pre-attention LSTM in the decoder and if you are done with the translation. Also notice that a `temperature` variable is introduced which determines how to select the next token given the predicted logits:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "522f6b6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
    "    \"\"\"Generates the next token in the sequence\n",
    "\n",
    "    Args:\n",
    "        decoder (Decoder): The decoder\n",
    "        context (tf.Tensor): Encoded sentence to translate\n",
    "        next_token (tf.Tensor): The predicted next token\n",
    "        done (bool): True if the translation is complete\n",
    "        state (list[tf.Tensor, tf.Tensor]): Hidden states of the pre-attention LSTM layer\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(tf.Tensor, np.float, list[tf.Tensor, tf.Tensor], bool): The next token, log prob of said token, hidden state of LSTM and if translation is done\n",
    "    \"\"\"\n",
    "    # Get the logits and state from the decoder\n",
    "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
    "    \n",
    "    # Trim the intermediate dimension \n",
    "    logits = logits[:, -1, :]\n",
    "        \n",
    "    # If temp is 0 then next_token is the argmax of logits\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "        \n",
    "    # If temp is not 0 then next_token is sampled out of logits\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "    \n",
    "    # Trim dimensions of size 1\n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "    \n",
    "    # Get the logit of the selected next_token\n",
    "    logit = logits[next_token].numpy()\n",
    "    \n",
    "    # Reshape to (1,1) since this is the expected shape for text encoded as TF tensors\n",
    "    next_token = tf.reshape(next_token, shape=(1,1))\n",
    "    \n",
    "    # If next_token is End-of-Sentence token you are done\n",
    "    if next_token == eos_id:\n",
    "        done = True\n",
    "    \n",
    "    return next_token, logit, state, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d2d76",
   "metadata": {},
   "source": [
    "See how it works by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9937547a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [[10026]]\n",
      "Logit: -18.7985\n",
      "Done? False\n"
     ]
    }
   ],
   "source": [
    "# PROCESS SENTENCE TO TRANSLATE AND ENCODE\n",
    "\n",
    "# A sentence you wish to translate\n",
    "eng_sentence = \"I love languages\"\n",
    "\n",
    "# Convert it to a tensor\n",
    "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
    "\n",
    "# Vectorize it and pass it through the encoder\n",
    "context = english_vectorizer(texts).to_tensor()\n",
    "context = encoder(context)\n",
    "\n",
    "# SET STATE OF THE DECODER\n",
    "\n",
    "# Next token is Start-of-Sentence since you are starting fresh\n",
    "next_token = tf.fill((1,1), sos_id)\n",
    "\n",
    "# Hidden and Cell states of the LSTM can be mocked using uniform samples\n",
    "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
    "\n",
    "# You are not done until next token is EOS token\n",
    "done = False\n",
    "\n",
    "# Generate next token\n",
    "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
    "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170323dd",
   "metadata": {},
   "source": [
    "<a name=\"ex5\"></a>\n",
    "## Exercise 5 - translate\n",
    "\n",
    "Now you can put everything together to translate a given sentence. For this, complete the `translate` function below. This function will take care of the following steps: \n",
    "- Process the sentence to translate and encode it\n",
    "\n",
    "+ Set the initial state of the decoder\n",
    "\n",
    "- Get predictions of the next token (starting with the \\<SOS> token) for a maximum of iterations (in case the \\<EOS> token is never returned)\n",
    "    \n",
    "+ Return the translated text (as a string), the logit of the last iteration (this helps measure how certain was that the sequence was translated in its totality) and the translation in token format.\n",
    "\n",
    "\n",
    "Hints: \n",
    "\n",
    "- The previous cell provides a lot of insights on how this function should work, so if you get stuck refer to it.\n",
    "\n",
    "+ Some useful docs:\n",
    "    + [tf.newaxis](https://www.tensorflow.org/api_docs/python/tf#newaxis)\n",
    "\n",
    "    - [tf.fill](https://www.tensorflow.org/api_docs/python/tf/fill)\n",
    "\n",
    "    + [tf.zeros](https://www.tensorflow.org/api_docs/python/tf/zeros)\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE**: Due to randomness processes involving tensorflow training and weight initializing, the results below may vary a lot, even if you retrain your model in the same session. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42c74f1f",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: translate\n",
    "def translate(model, text, max_length=50, temperature=0.0):\n",
    "    \"\"\"Translate a given sentence from English to Portuguese\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained translator\n",
    "        text (string): The sentence to translate\n",
    "        max_length (int, optional): The maximum length of the translation. Defaults to 50.\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, np.float, tf.Tensor): The translation, logit that predicted <EOS> token and the tokenized translation\n",
    "    \"\"\"\n",
    "    # Lists to save tokens and logits\n",
    "    tokens, logits = [], []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # PROCESS THE SENTENCE TO TRANSLATE\n",
    "    \n",
    "    # Convert the original string into a tensor\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    \n",
    "    # Vectorize the text using the correct vectorizer\n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "    \n",
    "    # Get the encoded context (pass the context through the encoder)\n",
    "    # Hint: Remember you can get the encoder by using model.encoder\n",
    "    context = model.encoder(context)\n",
    "    \n",
    "    # INITIAL STATE OF THE DECODER\n",
    "    \n",
    "    # First token should be SOS token with shape (1,1)\n",
    "    next_token = tf.fill((1, 1), sos_id)\n",
    "    \n",
    "    # Initial hidden and cell states should be tensors of zeros with shape (1, UNITS)\n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "    \n",
    "    # You are done when you draw a EOS token as next token (initial state is False)\n",
    "    done = False\n",
    "\n",
    "    # Iterate for max_length iterations\n",
    "    for i in range(max_length):\n",
    "        # Generate the next token\n",
    "        try:\n",
    "            next_token, logit, state, done = generate_next_token(\n",
    "                decoder=model.decoder,\n",
    "                context=context,\n",
    "                next_token=next_token,\n",
    "                done=done,\n",
    "                state=state,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        except:\n",
    "             raise Exception(\"Problem generating the next token\")\n",
    "        \n",
    "        # If done then break out of the loop\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # Add next_token to the list of tokens\n",
    "        tokens.append(next_token)\n",
    "        \n",
    "        # Add logit to the list of logits\n",
    "        logits.append(logit)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Concatenate all tokens into a tensor\n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "    \n",
    "    # Convert the translated tokens into text\n",
    "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
    "    translation = translation.numpy().decode()\n",
    "    \n",
    "    return translation, logits[-1], tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525e8ba",
   "metadata": {},
   "source": [
    "Try your function with temperature of 0, which will yield a deterministic output and is equivalent to a greedy decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "daaea8c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.0\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu adoro linguas de vista as senhoras .\n",
      "Translation tokens:[[   9  564 1032   11 1037   38  421    4]]\n",
      "Logit: -0.389\n"
     ]
    }
   ],
   "source": [
    "# Running this cell multiple times should return the same output since temp is 0\n",
    "\n",
    "temp = 0.0 \n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05129b",
   "metadata": {},
   "source": [
    "Try your function with temperature of 0.7 (stochastic output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e0697db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu eu adoro idiomas de senhora .\n",
      "Translation tokens:[[  9   9 564 850  11 279   4]]\n",
      "Logit: -0.830\n"
     ]
    }
   ],
   "source": [
    "# Running this cell multiple times should return different outputs since temp is not 0\n",
    "# You can try different temperatures\n",
    "\n",
    "temp = 0.7\n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3a9ea35",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_translate(translate, trained_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba027524",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5. Minimum Bayes-Risk Decoding\n",
    "\n",
    "As mentioned in the lectures, getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
    "\n",
    "- Take several random samples\n",
    "+ Score each sample against all other samples\n",
    "- Select the one with the highest score\n",
    "\n",
    "You will be building helper functions for these steps in the following sections.\n",
    "\n",
    "With the ability to generate different translations by setting different temperature values you can do what you saw in the lectures and generate a bunch of translations and then determine which one is the best candidate. You will now do this by using the provided `generate_samples` function. This function will return any desired number of candidate translations alongside the log-probability for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62301cd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
    "    \n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        \n",
    "        _, logp, sample = translate(model, text, temperature=temperature)\n",
    "        \n",
    "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
    "        \n",
    "        log_probs.append(logp)\n",
    "                \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06bd792c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated tensor: [9, 564, 1032, 11, 514, 4] has logit: -1.052\n",
      "Translated tensor: [9, 564, 1032, 11, 576, 4] has logit: -0.255\n",
      "Translated tensor: [9, 564, 850, 11, 313, 11, 1037, 4] has logit: -2.890\n",
      "Translated tensor: [9, 564, 1032, 11, 313, 11, 576, 4] has logit: -0.041\n"
     ]
    }
   ],
   "source": [
    "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
    "\n",
    "for s, l in zip(samples, log_probs):\n",
    "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b10677",
   "metadata": {},
   "source": [
    "## Comparing Overlaps\n",
    "\n",
    "After generating multiple translations, the next step is to evaluate the quality of each one. One method to achieve this is by comparing each translation against the others.\n",
    "\n",
    "Various metrics can be used for this purpose, as discussed in the lectures, and experimentation with these metrics is encouraged. For this assignment, the focus will be on calculating scores for **unigram overlaps**.\n",
    "\n",
    "A commonly used and straightforward metric is the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index), which calculates the intersection over the union of two sets. The `jaccard_similarity` function computes this metric for any pair of candidate and reference translations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edb54a71",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "        \n",
    "    candidate_set = set(candidate)\n",
    "    reference_set = set(reference)\n",
    "    \n",
    "    common_tokens = candidate_set.intersection(reference_set)\n",
    "\n",
    "    all_tokens = candidate_set.union(reference_set)\n",
    "\n",
    "    overlap = len(common_tokens) / len(all_tokens)\n",
    "        \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc3384bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "\n",
    "js = jaccard_similarity(l1, l2)\n",
    "\n",
    "print(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2510e3d",
   "metadata": {},
   "source": [
    "<a name=\"ex6\"></a>\n",
    "## rouge1_similarity\n",
    "\n",
    "Jaccard similarity is good but a more commonly used metric in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as shown in the lectures, you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:\n",
    "\n",
    "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n",
    "\n",
    "For the implementation of the `rouge1_similarity` function you want to use the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) class from the Python standard library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb2e0a00",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def rouge1_similarity(candidate, reference):\n",
    "    \"\"\"Computes the ROUGE 1 score between two token lists\n",
    "\n",
    "    Args:\n",
    "        candidate (list[int]): Tokenized candidate translation\n",
    "        reference (list[int]): Tokenized reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: Overlap between the two token lists\n",
    "    \"\"\"\n",
    "    candidate_word_counts = Counter(candidate)\n",
    "    reference_word_counts = Counter(reference)\n",
    "    \n",
    "    overlap = 0\n",
    "    \n",
    "    for token in candidate_word_counts.keys():\n",
    "        \n",
    "        token_count_candidate = candidate_word_counts[token]\n",
    "        \n",
    "        token_count_reference = reference_word_counts[token]\n",
    "        \n",
    "        overlap += min(token_count_candidate, token_count_reference)\n",
    "\n",
    "    precision = overlap/len(candidate)\n",
    "\n",
    "    recall = overlap/len(reference)\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "        f1_score = 2 *((precision * recall)/(precision + recall))\n",
    "        \n",
    "        return f1_score\n",
    "\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "14bb5295",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "\n",
    "r1s = rouge1_similarity(l1, l2)\n",
    "\n",
    "print(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8a058",
   "metadata": {},
   "source": [
    "## Computing the Overall Score\n",
    "\n",
    "\n",
    "The task now is to develop a function to calculate the overall score for a given sample. As explained in the lectures, this involves comparing each sample with all other samples. For example, if 30 sentences are generated, sentence 1 needs to be compared with sentences 2 through 30. Then, sentence 2 is compared with sentences 1 and 3 through 30, and so on. The average score of all comparisons will provide the overall score for each sample. To illustrate, the following steps outline how to compute the scores for a list of 4 samples:\n",
    "\n",
    "Compute the similarity score between sample 1 and sample 2\n",
    "Compute the similarity score between sample 1 and sample 3\n",
    "Compute the similarity score between sample 1 and sample 4\n",
    "Calculate the average score of these comparisons to determine the overall score for sample 1\n",
    "Repeat the process for samples 2 through 4 to obtain their overall scores.\n",
    "The results will be stored in a dictionary for convenient lookups.\n",
    "\n",
    "<a name=\"ex7\"></a>\n",
    "## average_overlap\n",
    "\n",
    "Complete the `average_overlap` function below which should implement the process described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "142264ff",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def average_overlap(samples, similarity_fn):\n",
    "    \"\"\"Computes the arithmetic mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        samples (list[list[int]]): Tokenized version of translated sentences\n",
    "        similarity_fn (Function): Similarity function used to compute the overlap\n",
    "\n",
    "    Returns:\n",
    "        dict[int, float]: A dictionary mapping the index of each translation to its score\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        overlap = 0\n",
    "        \n",
    "        for index_sample, sample in enumerate(samples):\n",
    "\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            overlap += sample_overlap\n",
    "\n",
    "\n",
    "        score = overlap / (len(samples) - 1)\n",
    "\n",
    "        score = round(score, 3)\n",
    "        \n",
    "        scores[index_candidate] = score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f36cf403",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n",
      "\n",
      "{0: 0.45, 1: 0.625, 2: 0.575}\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "\n",
    "avg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n",
    "\n",
    "print(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d961a304-7c03-4ecb-ba5f-c8747ed3ec39",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average overlap between lists: [1, 2, 3], [1, 4], [1, 2, 4, 5] and [5, 6] using Rouge1 similarity is:\n",
      "\n",
      "{0: 0.324, 1: 0.356, 2: 0.524, 3: 0.111}\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "l4 = [5,6]\n",
    "\n",
    "avg_ovlp = average_overlap([l1, l2, l3, l4], rouge1_similarity)\n",
    "\n",
    "print(f\"average overlap between lists: {l1}, {l2}, {l3} and {l4} using Rouge1 similarity is:\\n\\n{avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4482249",
   "metadata": {},
   "source": [
    "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. This is implemented in the `weighted_avg_overlap` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "398714be",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        \n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "\n",
    "            # Skip if the candidate index is the same as the sample index            \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # Convert log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "\n",
    "            weight_sum += sample_p\n",
    "\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            overlap += sample_p * sample_overlap\n",
    "            \n",
    "        score = overlap / weight_sum\n",
    "\n",
    "        score = round(score, 3)\n",
    "\n",
    "        scores[index_candidate] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e3dfd6d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted average overlap using Jaccard similarity is:\n",
      "\n",
      "{0: 0.443, 1: 0.631, 2: 0.558}\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "log_probs = [0.4, 0.2, 0.5]\n",
    "\n",
    "w_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n",
    "\n",
    "print(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b249f5",
   "metadata": {},
   "source": [
    "## MBR Decode\n",
    "The final step involves integrating all the components into the mbr_decode function. This function serves as a wrapper around the various elements developed previously.\n",
    "\n",
    "The mbr_decode function allows for experimentation with different numbers of samples, temperatures, and similarity functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfa640",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
    "    \n",
    "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
    "    \n",
    "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
    "\n",
    "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
    "    \n",
    "    max_score_key = max(scores, key=lambda k: scores[k])\n",
    "    \n",
    "    translation = decoded_translations[max_score_key]\n",
    "    \n",
    "    return translation, decoded_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "99507fcc-7727-45e7-933b-d3a08034f731",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation candidates:\n",
      "adoro linguas estrangeiras da nova forma .\n",
      "eu adoro linguas de novo .\n",
      "eu adoro idiomas de sorte .\n",
      "eu adoro linguas de vista as senhoras .\n",
      "adoro linguas em seguranca .\n",
      "eu adoro linguas de aviao .\n",
      "eu eu adoro linguas de vista de senhora .\n",
      "eu adoro idiomas de menos a senhora .\n",
      "adoro linguas de usuario .\n",
      "eu adoro idiomas de idade .\n",
      "\n",
      "Selected translation: eu adoro linguas de novo .\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"I love languages\"\n",
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
